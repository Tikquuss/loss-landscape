{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T2AxvullI6yB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model & Dataset"
      ],
      "metadata": {
        "id": "cY4BPb0Np9N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Tikquuss/mlp_grokking"
      ],
      "metadata": {
        "id": "6yeKquxoI-jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mlp_grokking"
      ],
      "metadata": {
        "id": "bjDSwNqXJJ44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "s_FmbpKMJMWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.modeling import Model\n",
        "from src.dataset import get_dataloader\n",
        "from src.trainer import train\n",
        "from src.utils import AttrDict"
      ],
      "metadata": {
        "id": "vJzobOuh9MO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pct=80\n",
        "weight_decay=0.0\n",
        "representation_lr=0.001\n",
        "decoder_lr=0.001\n",
        "representation_dropout=0.0\n",
        "decoder_dropout=0.0\n",
        "opt=\"adam\"\n",
        "\n",
        "#group_name=f\"tdf={train_pct}-wd={weight_decay}-r_lr={representation_lr}-d_lr={decoder_lr}-r_d={representation_dropout}-d_d={decoder_dropout}-opt={opt}\"\n",
        "group_name=f\"1\"\n",
        "\n",
        "\n",
        "random_seed=0\n",
        "operator=\"+\"\n",
        "modular=False\n",
        "\n",
        "log_dir=\"../log_files\"\n",
        "\n",
        "p = 10\n",
        "task = \"classification\"\n",
        "\n",
        "params = AttrDict({\n",
        "    ### Main parameters\n",
        "    \"task\" : task,\n",
        "    \"exp_id\" : f\"{task}_{group_name}\",\n",
        "    \"log_dir\" : f\"{log_dir}/{random_seed}\",\n",
        "\n",
        "    ### Model\n",
        "    \"emb_dim\" : 8, \n",
        "    \"hidden_dim\" : 16,  \n",
        "    \"n_layers\" : 1,\n",
        "\t  \"representation_dropout\" : representation_dropout,\n",
        "\t  \"decoder_dropout\" : decoder_dropout,\n",
        "    \"pad_index\" : None, \n",
        "    \"p\" : p, \n",
        "\n",
        "    ### Dataset\n",
        "    \"operator\" : operator, \n",
        "    \"modular\" : modular,\n",
        "    \"train_pct\" : train_pct,\n",
        "    \"batch_size\" : 512,\n",
        "\n",
        "    ### Optimizer\n",
        "    \"optimizer\" : f\"{opt},weight_decay={weight_decay},beta1=0.9,beta2=0.99,eps=0.00000001\",\n",
        "    \"representation_lr\" : representation_lr,\n",
        "    \"decoder_lr\" : decoder_lr,\n",
        "\n",
        "    ### LR Scheduler\n",
        "    \"lr_scheduler\" : None,\n",
        "    #\"lr_scheduler\" : \"reduce_lr_on_plateau,factor=0.2,patience=20,min_lr=0.00005,mode=min,monitor=val_loss\",\n",
        "    \n",
        "    ### Training\n",
        "    \"max_epochs\" : 10, \n",
        "    \"validation_metrics\" : \"val_loss\",\n",
        "    \"checkpoint_path\" : None, \n",
        "    \"model_name\": \"\", \n",
        "    \"every_n_epochs\":25, \n",
        "    \"every_n_epochs_show\":25, \n",
        "    \"early_stopping_patience\":1e9, \n",
        "    \"save_top_k\":-1,\n",
        "\n",
        "    # Wandb \n",
        "    \"use_wandb\" : False,\n",
        "    \"wandb_entity\" : \"grokking_ppsp\",\n",
        "    \"wandb_project\" : f\"toy_model_grokking_op={operator}-p={p}-task={task}-mod={modular}\",\n",
        "    \"group_name\" : group_name,\n",
        "\n",
        "    \"group_vars\" : None,\n",
        "\n",
        "    ### Intrinsic Dimension Estimation\n",
        "    \"ID_params\" : {},\n",
        "    #\"ID_params\": {\"method\" : \"mle\", \"k\":2},\n",
        "    #\"ID_params\": {\"method\" : \"twonn\"},\n",
        "    \n",
        "    # Devices & Seed\n",
        "    \"accelerator\" : \"auto\",\n",
        "    \"devices\" : \"auto\",\n",
        "    \"random_seed\": random_seed,\n",
        "\n",
        "    ### Early_stopping (for grokking) : Stop the training `patience` epochs after the `metric` has reached the value `metric_threshold` \n",
        "    #\"early_stopping_grokking\" : None,\n",
        "    \"early_stopping_grokking\" : \"patience=int(1000),metric=str(val_acc),metric_threshold=float(90.0)\",\n",
        "\n",
        "})\n",
        "params[\"weight_decay\"] = weight_decay\n",
        "params[\"regression\"] = task == \"regression\"\n",
        "train_loader, val_loader, dataloader, data_infos = get_dataloader(\n",
        "    p, train_pct, regression = params.regression, operator=params.operator, \n",
        "    modular = params.modular, batch_size=params.batch_size, num_workers=2\n",
        ")\n",
        "print(data_infos)\n",
        "params[\"data_infos\"] = data_infos"
      ],
      "metadata": {
        "id": "nzn5Ocrh7Xl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, result = train(params, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "wUzG0ScF74r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#! rm -r /content/log_files/0"
      ],
      "metadata": {
        "id": "LJgNIZ7xMiB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/log_files/0/classification_1/lightning_logs"
      ],
      "metadata": {
        "id": "_dLC6B-P6V7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re \n",
        "\n",
        "def sorted_nicely(l): \n",
        "    \"\"\" Sort the given iterable in the way that humans expect.\n",
        "    https://stackoverflow.com/a/2669120/11814682\n",
        "    \"\"\" \n",
        "    convert = lambda text: int(text) if text.isdigit() else text \n",
        "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
        "    return sorted(l, key = alphanum_key)"
      ],
      "metadata": {
        "id": "Yks5lthEygsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "rEYrqIXfMRBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_folder = \"/content/log_files/0/classification_1\"\n",
        "\n",
        "model_files = os.listdir(pretrained_folder)\n",
        "model_files = [f for f in model_files if re.match('^epoch=[0-9]+-val_loss=[0-9]+\\.[0-9]+.ckpt$', f)]\n",
        "model_files = sorted_nicely(model_files)\n",
        "model_files = [pretrained_folder + \"/\" + f for f in model_files]\n",
        "\n",
        "model_files"
      ],
      "metadata": {
        "id": "9vlv_zRFyvZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model.load_from_checkpoint(model_files[0])"
      ],
      "metadata": {
        "id": "qLAeJ4UyK4Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "id": "8iZh3WaxkYqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loss-landscape"
      ],
      "metadata": {
        "id": "ZH-V2C7Nkd1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! rm -r loss-landscape"
      ],
      "metadata": {
        "id": "6oRqXDguuN2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Tikquuss/loss-landscape"
      ],
      "metadata": {
        "id": "Q8HC5ojzkg8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd loss-landscape"
      ],
      "metadata": {
        "id": "fV4pfLVKkj0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Dsu4tJejkk8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import AttrDict"
      ],
      "metadata": {
        "id": "SCziG5Ypk9WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lightning_module_class = Model"
      ],
      "metadata": {
        "id": "JU6dSX2tnnse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## code"
      ],
      "metadata": {
        "id": "BebOT81EEflX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### plot_surface"
      ],
      "metadata": {
        "id": "9dJ-eSzAWFAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from plot_surface import plot_surface"
      ],
      "metadata": {
        "id": "upOZZ43up113"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = AttrDict({ \n",
        "    \n",
        "    'mpi' : True, # use cuda\n",
        "    'cuda' : False, # use mpi\n",
        "    'threads' : 2, # 'number of threads'\n",
        "    'ngpu' : 1, # 'number of GPUs to use for each rank, useful for data parallel evaluation\n",
        "\n",
        "    # data parameters\n",
        "\n",
        "    'raw_data' :False, # 'no data preprocessing'\n",
        "    'data_split' : 1, #'the number of splits for the dataloader')\n",
        "    'split_idx' : 0, # 'the index of data splits for the dataloader'\n",
        "\n",
        "    # model parameters\n",
        "    \n",
        "    # parser.add_argument('--model', default='resnet56', help='model name')\n",
        "    # parser.add_argument('--model_folder', default='', help='the common folder that contains model_file and model_file2')\n",
        "    'model_file' : model_files[0], # path to the trained model file\n",
        "    #'model_file2' : model_files[-1], # use (model_file2 - model_file) as the xdirection\n",
        "    'model_file2' : \"\", # use (model_file2 - model_file) as the xdirection\n",
        "    'model_file3' : \"\", # use (model_file2 - model_file) as the xdirection\n",
        "    #'loss_name' : 'crossentropy', # help='loss functions: crossentropy | mse')\n",
        "\n",
        "    # direction parameters\n",
        "\n",
        "    'dir_file' : '',  # 'specify the name of direction file, or the path to an eisting direction file\n",
        "    'dir_type' : 'weights', #'direction type: weights | states (including BN\\'s running_mean/var)'\n",
        "    'x' : '-1:1:51', #'A string with format xmin:x_max:xnum'\n",
        "    #'y' : None, #'A string with format ymin:ymax:ynum'\n",
        "    'y' : '-1:1:51', #'A string with format ymin:ymax:ynum'\n",
        "    'xnorm' : '', # 'direction normalization: filter | layer | weight'\n",
        "    'ynorm' : '', # 'direction normalization: filter | layer | weight'\n",
        "    'xignore' : '', #'ignore bias and BN parameters: biasbn'\n",
        "    'yignore' : '', #'ignore bias and BN parameters: biasbn'\n",
        "    'same_dir' : False, # 'use the same random direction for both x-axis and y-axis'\n",
        "    'idx' : 0, # 'the index for the repeatness experiment')\n",
        "    'surf_file' : '', # 'customize the name of surface file, could be an existing file.'\n",
        "\n",
        "    # plot parameters\n",
        "\n",
        "    'proj_file' : '', # 'the .h5 file contains projected optimization trajectory.'\n",
        "    'loss_max' : 40, # 'Maximum value to show in 1D plot'\n",
        "    'vmax' : 10, # 'Maximum value to map'\n",
        "    'vmin' : 0.1, # 'Miminum value to map'\n",
        "    'vlevel' : 0.5, # 'plot contours every vlevel'\n",
        "    'show' : True, # 'show plotted figures'\n",
        "    'log' : False, # 'use log scale for loss values'\n",
        "    'plot' : True, # 'plot figures after computation'\n",
        "})"
      ],
      "metadata": {
        "id": "2sHQdSjbrMEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = train_loader\n",
        "#dataloader = val_loader\n",
        "\n",
        "dir_file, surf_file = plot_surface(args, lightning_module_class, dataloader, metrics = ['test_loss', 'test_acc'])"
      ],
      "metadata": {
        "id": "a_FPS-hlN9TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### plot_trajectory"
      ],
      "metadata": {
        "id": "hY7pn0UXA9yN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from plot_trajectory import plot_trajectory"
      ],
      "metadata": {
        "id": "H3uLKN-Lnqps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = AttrDict({ \n",
        "    'model_folder' : pretrained_folder, # 'folders for models to be projected'\n",
        "    'dir_type' : 'weights', #\"\"\"direction type: weights (all weights except bias and BN paras) states (include BN.running_mean/var)\"\"\")\n",
        "    'ignore' : '', # 'ignore bias and BN paras: biasbn (no bias or bn)')'\n",
        "    'save_epoch' : 1, # 'save models every few epochs')\n",
        "\n",
        "    'dir_file' : '', #'load the direction file for projection')\n",
        "})"
      ],
      "metadata": {
        "id": "OxM1JafloQLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#os.remove(\"/content/log_files/0/classification_1/PCA_weights_save_epoch=1/directions.h5_proj_cos.h5\")"
      ],
      "metadata": {
        "id": "dIxuIDB_0KXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proj_file, dir_file = plot_trajectory(args, model_files, lightning_module_class)"
      ],
      "metadata": {
        "id": "HiFy94lWoLWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### plot_hessian_eigen"
      ],
      "metadata": {
        "id": "HATybnU8pwzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from plot_hessian_eigen import plot_hessian_eigen"
      ],
      "metadata": {
        "id": "gbp--FR2o97f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = AttrDict({ \n",
        "    \n",
        "    'mpi' : True, # use cuda\n",
        "    'cuda' : False, # use mpi\n",
        "    'threads' : 2, # 'number of threads'\n",
        "    'ngpu' : 1, # 'number of GPUs to use for each rank, useful for data parallel evaluation\n",
        "\n",
        "    # data parameters\n",
        "\n",
        "    'raw_data' :False, # 'no data preprocessing'\n",
        "    'data_split' : 1, #'the number of splits for the dataloader')\n",
        "    'split_idx' : 0, # 'the index of data splits for the dataloader'\n",
        "\n",
        "    # model parameters\n",
        "    \n",
        "    # parser.add_argument('--model', default='resnet56', help='model name')\n",
        "    # parser.add_argument('--model_folder', default='', help='the common folder that contains model_file and model_file2')\n",
        "    'model_file' : model_files[0], # path to the trained model file\n",
        "    'model_file2' : model_files[-1], # use (model_file2 - model_file) as the xdirection\n",
        "    'model_file3' : \"\", # use (model_file2 - model_file) as the xdirection\n",
        "    #'loss_name' : 'crossentropy', # help='loss functions: crossentropy | mse')\n",
        "\n",
        "    # direction parameters\n",
        "\n",
        "    'dir_file' : '',  # 'specify the name of direction file, or the path to an eisting direction file\n",
        "    'dir_type' : 'weights', #'direction type: weights | states (including BN\\'s running_mean/var)'\n",
        "    'x' : '-1:1:51', #'A string with format xmin:x_max:xnum'\n",
        "    'y' : None, #'A string with format ymin:ymax:ynum'\n",
        "    #'y' : '-1:1:51', #'A string with format ymin:ymax:ynum'\n",
        "    'xnorm' : '', # 'direction normalization: filter | layer | weight'\n",
        "    'ynorm' : '', # 'direction normalization: filter | layer | weight'\n",
        "    'xignore' : '', #'ignore bias and BN parameters: biasbn'\n",
        "    'yignore' : '', #'ignore bias and BN parameters: biasbn'\n",
        "    'same_dir' : False, # 'use the same random direction for both x-axis and y-axis'\n",
        "    'idx' : 0, # 'the index for the repeatness experiment')\n",
        "    'surf_file' : '', # 'customize the name of surface file, could be an existing file.'\n",
        "\n",
        "    # plot parameters\n",
        "\n",
        "    'show' : True, # help='show plotted figures')\n",
        "    'plot' : True, #  help='plot figures after computation')\n",
        "})"
      ],
      "metadata": {
        "id": "m2h-ioSi_m33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = train_loader\n",
        "#dataloader = val_loader"
      ],
      "metadata": {
        "id": "uokDE7EipR1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss(pl_module, batch):\n",
        "    \"\"\"\n",
        "    Given a batch of data, this function returns the  loss\n",
        "    \"\"\"    \n",
        "    x, y = batch\n",
        "    tensor, _, _ = pl_module.forward(x)\n",
        "    loss = pl_module.criterion(input = tensor, target=y)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "dDl1iSsOpiKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_file, surf_file = plot_hessian_eigen(args, lightning_module_class, dataloader, get_loss)"
      ],
      "metadata": {
        "id": "fNIJk1k7o7ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## from scratsh"
      ],
      "metadata": {
        "id": "_I_LZGcGWjKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import parameters_to_vector, vector_to_parameters"
      ],
      "metadata": {
        "id": "od4v6-ZqWtBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta = parameters_to_vector(model.parameters())"
      ],
      "metadata": {
        "id": "wqJPco7FO409"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = Model(params)\n",
        "vector_to_parameters(theta, model_2.parameters())"
      ],
      "metadata": {
        "id": "x9rDr3MPO769"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}